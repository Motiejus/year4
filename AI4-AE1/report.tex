\documentclass[english,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{verbatim}
\usepackage{hyperref}
\numberwithin{equation}{section}
\newcommand{\naive}{na\"{\i}ve\ }
\newcommand{\Naive}{Na\"{\i}ve\ }
\newcommand{\listing}[1]{
    \subsubsection{#1}
    {\small \verbatiminput{#1}}
}

\input{res/report_data.tex}

\begin{document}

\title{Artificial Intelligence 4 Assessed Exercise}
\author{Motiejus Jak≈°tys}
\date{23 November 2012}

\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
\subsection{Identification}
This document is the report of the Artificial Intelligence 4 Assessed
Exercise.

\subsection{Contents of the deliverable}

TODO

\section{Design}
There were 100 audio files given: 50 containing silence, and 50 containing
speech. The purpose of the exercise was to create a system which predicts the
class of a given audio file (silence or speech) using a training set. The
system was created and evaluated. How it works and its performance is described
in this document.

\subsection{Performance}
Performance measure of the agent by the success rate of the matching
process. The more files are correctly assigned to the silence or speech
category, the better the performance.

\subsection{Environment}
Agent operates in any environment which requires distinction between silence
and speech in a sound file. Most typically it is Telephone Exchange.

\subsection{Actuators}
Actuators for this agent are computer screen or output file. This agent is
likely to be a part of a larger program, which would execute a more
business-oriented task, like stop a phone conversation or contribute to the
database with a statistical property of the phone conversation.

\subsection{Sensors}
Since this agent is purely software agent, its sensors are sound files.


\section{Theory}

This work consisted of two sub-tasks:
\begin{description}
    \item{Sensing:} reading the sample files, extracting energy, magnitude and
        zero crossing rate (later ZCR).

    \item{Reasoning:} applying \naive Bayes classifier to the data and
        predicting the class (speech or silence) of the given file.
\end{description}

\subsection{Sensing}

\subsubsection{Extracting Energy, Magnitude, ZCR}

At first a program was made which can extract energy, magnitude and ZCR from a
given audio file. This is the first step in sensing: extract necessary
properties from raw data. In this work all three properties were calculated
using a window of 240 samples (in all the equations below, $w$ is a rectangular
window, $n$ is $240$). Here you will find energy, magnitude and ZCR described.
Each property has an associated figure with a provided sample.

Energy (fig.~\ref{fig:sample_e}) is calculated as follows:

$$ E[n] = \sum_{k=-\infty}^{\infty} s^2[k] \cdot w[n-k] $$

Magnitude (fig.~\ref{fig:sample_m}):

$$ M[n] = \sum_{k=-\infty}^{\infty} |s[k]| \cdot w[n-k] $$

Zero crossing rate (fig.~\ref{fig:sample_z}):

$$
    Z[n] = \frac{1}{2N}
        \sum_{k=-\infty}^{\infty}
        |sign(s[k]) - sign(s[k-1])| \cdot w[n-m]
$$

Implementations of these functions can be found in a file TODO in
appendix TODO.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/sample_e.pdf}
        \caption{Energy}
        \label{fig:sample_e}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/sample_m.pdf}
        \caption{Magnitude}
        \label{fig:sample_m}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/sample_z.pdf}
        \caption{ZCR}
        \label{fig:sample_z}
    \end{subfigure}
    \caption{Sample audio file}\label{fig:sample}
\end{figure}

\subsubsection{Aggregating sample data}
\label{sec:aggregating}

Once it was possible to calculate energy, magnitude and ZCR, it was necessary
to aggregate the data. A value by itself does not give much information,
however, mean value of the file is a bit more valuable property. Once means of
every property for every file are known, correlation between those can be
visualised and measured.

Pearson correlation for energy--magnitude is $\correlationem$
(fig.~\ref{fig:aggr_e-m}), for energy--ZCR is $\correlationez$
(fig.~\ref{fig:aggr_e-z}) and magnitude--ZCR is $\correlationmz$
(fig.~\ref{fig:aggr_m-z}).

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/aggr_e-m.pdf}
        \caption{Energy -- Magnitude}
        \label{fig:aggr_e-m}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/aggr_e-z.pdf}
        \caption{Energy -- ZCR}
        \label{fig:aggr_e-z}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/aggr_m-z.pdf}
        \caption{Magnitude -- ZCR}
        \label{fig:aggr_m-z}
    \end{subfigure}
    \caption{Aggregated data}\label{fig:aggr}
\end{figure}

\subsubsection{Training and test set}

The goal is to apply \naive Bayes classification to the data set, and decide
the class of the unknown file, whether it is silence or speech. We (strongly)
assume that neither of the properties that we are using in the classification
are dependant. For the first run let us close our eyes to the
$\correlationem$ linear correlation between energy and magnitude, run the
analysis and measure the performance. Later the most conflicting metric will be
discarded, learning and analysis will be re-ran, and performance will be
compared.

It will be done as follows: it is given 100 audio samples, 50 with silence and
50 with speech. A single test run takes first 5 samples of silence, first 5
samples of silence, applies idiot Bayes model\cite{idiotbayes} and gets the
probability of the sample being silence or speech. There will be 10 test
executions, so every file will be assigned a probability distribution.

\subsubsection{Classification}

Probability of an unknown file being silence or speech is the same:

$$ P(silence) = P(speech) = 0.5 $$

It is possible to determine probability distribution of $P(C|E)$, $P(C|M)$,
$P(C|Z)$. Our goal is to calculate $ P(C | E, M, Z), C = \left\{ silence,
speech \right\}$ from that data. In other words, calculate probability of the
file being silence, given energy, magnitude and ZCR, given our training set. In
order to get $P(cause|effect)$ (in this case cause is energy, magnitude and
ZCR, and effect is silence or speech), we have to apply Bayes and chain rules:

\begin{align}
   & P(C | E, M, Z) = \\
   & \frac{ P(E|C,M,Z) P(C|M,Z) }{ P(E|M,Z) } = \\
   & \frac{ P(E|C,M,Z) P(M|C,Z) P(C|Z) }{ P(E|M,Z) P(M|Z) } = \\
   & \frac{ P(E|C,M,Z) P(M|C,Z) P(Z|C) P(C) }{ P(E|M,Z) P(M|Z) P(Z) } =
    \label{eq:chain_rule} \\
   & \alpha P(E|C,M,Z) P(M|C,Z) P(Z|C) P(C) = \label{eq:alpha} \\
   & \alpha P(E|C) P(M|C) P(Z|C) P(C) \label{eq:independence}
\end{align}

Getting to equation~\ref{eq:chain_rule} is just applying chain rule. Since the
equation denominator is a constant (does not depend on $C$), it is equal in
both classes (silence and speech), and hence we assign it a new name $\alpha$.
Getting to equation~\ref{eq:independence} is slightly more interesting. Like
mentioned before, we assume that all properties -- energy, magnitude and ZCR --
are conditionally independent. For that reason we can simplify the mentioned
expression using \naive Bayesian classifier and $P(K | C, \mathbf{x})$ becomes
$P(K|C)$ ($K = \left\{E, M, Z\right\}, C = \left\{ silence, speech \right\}$).

$P(C)$ is $0.5$, because it is known that exactly half of the files are speech,
and the other half are silence. It is trivial to calculate $\sigma_{c}^{2}$,
standard deviation of energy values in the given class, and $\mu_{c}$ is the
mean of the same set of values. Now suppose we have a value of energy $v$ and
want to calculate the probability distribution of it being in class silence or
speech. Normal probability distribution for property $K = \left\{E, M,
Z\right\}$ can be calculated as follows:

$$
P(K = v|c) = \frac{1}{\sqrt{2 \pi \sigma_{c}^{2}}}
e ^{- \frac{ (v - \mu_{c})^2 }{2 \sigma^2} }
$$

Now just calculating values of $P(C=silence| E, M, Z)$ are just putting the
numbers into equation \ref{eq:independence}. This explanation completes the
theory how to get a probability distribution of an unknown file. Now some
experiment results will be presented.

\section{Experiments}

\subsection{\Naive Bayesian classification using Energy, Magnitude and ZCR}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{res/predictions.pdf}
    \caption{Prediction performance}
    \label{fig:simple_performance}
\end{figure}

Like it can be seen in figure~\ref{fig:simple_performance}, all records with
silence were predicted properly, but five records with speech were
mis-predicted as silence. See more details about performance in
table~\ref{tab:correlation_table}. Value in "silence correct mean" means that
the average correct prediction for a silence sample is $\sicorrectmeanemz$. This
means that our Bayesian classifier was quite determined about the class of the
file. However, value in "speech incorrect classifications" means that actual
speech samples were given average probability being speech of
$\spincorrectmeanemz$. It is quite easy to see the five mispredicted files in
figure~\ref{fig:aggr}.

\begin{figure}
    \centering
    \caption{Correlation table using Energy, Magnitude and ZCR}
    \correlationtableemz
    \label{tab:correlation_table_emz}
\end{figure}

\subsection{\Naive Bayesian classification using Energy and ZCR}

Like mentioned in section~\ref{sec:aggregating}, energy and magnitude have very
high correlation. Even for \naive Bayesian classification I think using two
properties with Pearson correlation $\correlationem$ is too much. In this
section one of these classifiers is removed, the same experiment is re-done,
and performance is re-evaluated. Which parameter of those two should be
removed? Energy or magnitude? Because Energy-ZCR correlation is lower than
Magnitude-ZCR, magnitude is removed. That way we have lower correlation between
the two variables.

Table~\ref{tab:correlation_table_ez} shows the performance of the new matching
algorithm which uses only Energy and ZCR. It is not much different than using
three of them, however, it yields slightly better performance in estimating
speech. In other words, it gives higher probability for speech records. On the
other hand, new algorithm returns a slightly smaller probability for silence
prediction (though it still correctly predicts silence).

\begin{figure}
    \centering
    \caption{Correlation table using Energy and ZCR}
    \correlationtableez
    \label{tab:correlation_table_ez}
\end{figure}

\section{Conclusions}

This report describes an experiment which uses \naive Bayes classification
to decide whether a given audio file is silence or speech. Performance of the
algorithm using sample data is $95\%$.

There have been two experiments. First one used Energy, Magnitude and ZCR for
classification. The second one used only Energy and Magnitude. Performance of
the second algorithm is the same as the first one. However, if we include the
level of uncertainty for wrong estimations as performance measurement, second
experiment yielded slightly better results. In other words, the second algorithm
was "by tiny fraction less wrong".

\section{Appendix}

The whole code repository is available on github: \\
\url{https://github.com/Motiejus/year4/tree/master/AI4-AE1}

\subsection{Core programs}
These scripts are the core of the program. They do sensing and analysis.

\listing{sensing.py}
\listing{analyze.py}

\subsection{Miscelanous files}

These scripts are helpers for core program execution, plotting, and report
generation.

\listing{plot.sh}
\listing{report.py}
\listing{Makefile}
\listing{samples/Makefile}

\clearpage
\begin{thebibliography}{9}

    \bibitem{idiotbayes}
        Russell, Stuart J.; Norvig, Peter (2003),
        \emph{Artificial Intelligence: A Modern Approach (2nd ed.)},
        Upper Saddle River, New Jersey: Prentice Hall,
        ISBN 0-13-604259-4, p. 499.

\end{thebibliography}
\end{document}
