\documentclass[english,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{hyperref}
\newcommand{\naive}{na\"{\i}ve\ }
\numberwithin{equation}{section}

\input{res/report_data.tex}

\begin{document}

\title{Artificial Intelligence 4 Assessed Exercise}
\author{Motiejus Jak≈°tys}
\date{23 November 2012}

\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
\subsection{Identification}
This document is the report of the Artificial Intelligence 4 Assessed
Exercise.

\subsection{Contents of the deliverable}

TODO

\section{Design}
There were 100 audio files given: 50 containing silence, and 50 containing
speech. The purpose of the exercise was to create a system which predicts the
class of a given audio file (silence or speech) using a training set. The
system was created and evaluated. How it works and its performance is described
in this document.

\subsection{Performance}
Performance measure of the agent by the success rate of the matching
process. The more files are correctly assigned to the silence or speech
category, the better the performance.

\subsection{Environment}
Agent operates in any environment which requires distinction between silence
and speech in a sound file. Most typically it is Telephone Exchange.

\subsection{Actuators}
Actuators for this agent are computer screen or output file. This agent is
likely to be a part of a larger program, which would execute a more
business-oriented task, like stop a phone conversation or contribute to the
database with a statistical property of the phone conversation.

\subsection{Sensors}
Since this agent is purely software agent, its sensors are sound files.


\section{Theory}

This work consisted of two sub-tasks:
\begin{description}
    \item{Sensing:} reading the sample files, extracting energy, magnitude and
        zero crossing rate (later ZCR).

    \item{Reasoning:} applying \naive Bayes classifier to the data and
        predicting the class (speech or silence) of the given file.
\end{description}

\subsection{Sensing}

\subsubsection{Extracting Energy, Magnitude, ZCR}

At first a program was made which can extract energy, magnitude and ZCR from a
given audio file. This is the first step in sensing: extract necessary
properties from raw data. In this work all three properties were calculated
using a window of 240 samples (in all the equations below, $w$ is a rectangular
window, $n$ is $240$). Here you will find energy, magnitude and ZCR described.
Each property has an associated figure with a provided sample.

Energy (fig.~\ref{fig:sample_e}) is calculated as follows:

$$ E[n] = \sum_{k=-\infty}^{\infty} s^2[k] \cdot w[n-k] $$

Magnitude (fig.~\ref{fig:sample_m}):

$$ M[n] = \sum_{k=-\infty}^{\infty} |s[k]| \cdot w[n-k] $$

Zero crossing rate (fig.~\ref{fig:sample_z}):

$$
    Z[n] = \frac{1}{2N}
        \sum_{k=-\infty}^{\infty}
        |sign(s[k]) - sign(s[k-1])| \cdot w[n-m]
$$

Implementations of these fuctions can be found in a file TODO in
appendix TODO.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/sample_e.pdf}
        \caption{Energy}
        \label{fig:sample_e}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/sample_m.pdf}
        \caption{Magnitude}
        \label{fig:sample_m}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/sample_z.pdf}
        \caption{ZCR}
        \label{fig:sample_z}
    \end{subfigure}
    \caption{Sample audio file}\label{fig:sample}
\end{figure}

\subsubsection{Aggregating sample data}

Once it was possible to calculate energy, magnitude and ZCR, it was necessary
to aggregate the data. A value by itself does not give much information,
however, mean value of the file is a bit more valuable property. Once means of
every property for every file are known, correlation between those can be
visualised and measured.

Pearson correlation for energy--magnitude is $\correlationem$
(fig.~\ref{fig:aggr_e-m}), for energy--ZCR is $\correlationez$
(fig.~\ref{fig:aggr_e-z}) and magnitude--ZCR is $\correlationmz$
(fig.~\ref{fig:aggr_m-z}).

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/aggr_e-m.pdf}
        \caption{Energy -- Magnitude}
        \label{fig:aggr_e-m}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/aggr_e-z.pdf}
        \caption{Energy -- ZCR}
        \label{fig:aggr_e-z}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{res/aggr_m-z.pdf}
        \caption{Magnitude -- ZCR}
        \label{fig:aggr_m-z}
    \end{subfigure}
    \caption{Aggregated data}\label{fig:aggr}
\end{figure}

\subsubsection{Training and test set}

The goal is to apply \naive Bayes classification to the data set, and decide
the class of the unknown file, whether it is silence or speech. We (strongly)
assume that neither of the properties that we are using in the classification
are dependant. For the first run let us close our eyes to the
$\correlationem$ linear correlation between energy and magnitude, run the
analysis and measure the performance. Later the most conflicting metric will be
discarded, learning and analysis will be re-ran, and performance will be
compared.

It will be done as follows: it is given 100 audio samples, 50 with silence and
50 with speech. A single test run takes first 5 samples of silence, first 5
samples of silence, applies idiot Bayes model\cite{idiotbayes} and gets the
probability of the sample being silence or speech. There will be 10 test
executions, so every file will be assigned a probabilty distribution.

\subsubsection{Classification}

Probability of an unknown file being silence or speech is the same:

$$ P(silence) = P(speech) = 0.5 $$

It is possible to determine probability distribution of $P(C|E)$, $P(C|M)$,
$P(C|Z)$. Our goal is to calculate $ P(C | E, M, Z), C = \left\{ silence,
speech \right\}$ from that data. In other words, calculate probability of the
file being silence, given energy, magnitude and ZCR, given our training set. In
order to get $P(cause|effect)$ (in this case cause is energy, magnitude and
ZCR, and effect is silence or speech), we have to apply Bayes and chain rules:

\begin{align}
   & P(C | E, M, Z) = \\
   & \frac{ P(E|C,M,Z) P(C|M,Z) }{ P(E|M,Z) } = \\
   & \frac{ P(E|C,M,Z) P(M|C,Z) P(C|Z) }{ P(E|M,Z) P(M|Z) } = \\
   & \frac{ P(E|C,M,Z) P(M|C,Z) P(Z|C) P(C) }{ P(E|M,Z) P(M|Z) P(Z) } =
    \label{eq:chain_rule} \\
   & \alpha P(E|C,M,Z) P(M|C,Z) P(Z|C) P(C) = \label{eq:alpha} \\
   & \alpha P(E|C) P(M|C) P(Z|C) P(C) \label{eq:independence}
\end{align}

Getting to equation~\ref{eq:chain_rule} is just applying chain rule. Since the
equation denominator is a constant (does not depend on $C$), it is equal in
both classes (silence and speech), and hence we assign it a new name $\alpha$.
Getting to equation~\ref{eq:independence} is slightly more interesting. Like
mentioned before, we assume that all properties -- energy, magnitude and ZCR --
are conditionally independent. For that reason we can simplify the mentioned
expression using \naive Bayesian classifier and $P(K | C, \mathbf{x})$ becomes
$P(K|C)$ ($K = \left\{E, M, Z\right\}, C = \left\{ silence, speech \right\}$).

$P(C)$ is $0.5$, because it is known that exactly half of the files are speech,
and the other half are silence. It is trivial to calculate $\sigma_{c}^{2}$,
standard deviation of energy values in the given class, and $\mu_{c}$ is the
mean of the same set of values. Now suppose we have a value of energy $v$ and
want to calculate the probability distribution of it being in class silence or
speech. Normal probability distribution for property $K = \left\{E, M,
Z\right\}$ can be calculated as follows:

$$
P(K = v|c) = \frac{1}{\sqrt{2 \pi \sigma_{c}^{2}}}
e ^{- \frac{ (v - \mu_{c})^2 }{2 \sigma^2} }
$$

Now just calculating values of $P(C=silence| E, M, Z)$ are just putting the
numbers into equation \ref{eq:independence}. This explanation completes the
theory how to get a probability distribution of an unknown file. Now some
experiment results will be presented.

\section{Experiments}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{res/predictions.pdf}
    \caption{Prediction performance}
    \label{fig:simple_performance}
\end{figure}

Like it can be seen in figure~\ref{fig:simple_performance}, all records with
silence were predicted properly, but five records with speech were
mis-predicted as silence. See more details about performance in
table~\ref{tab:correlation_table}.

\begin{figure}
    \centering
    \caption{Correlation table}
    \correlationtable
    \label{tab:correlation_table}
\end{figure}

\clearpage
\begin{thebibliography}{9}

    \bibitem{idiotbayes}
        Russell, Stuart J.; Norvig, Peter (2003),
        \emph{Artificial Intelligence: A Modern Approach (2nd ed.)},
        Upper Saddle River, New Jersey: Prentice Hall,
        ISBN 0-13-604259-4, p. 499.

\end{thebibliography}
\end{document}
