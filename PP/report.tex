\documentclass[english,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{verbatim}
\usepackage{hyperref}
\newcommand{\listing}[1]{
    \subsubsection{#1}
    \label{lst:#1}
    {\small \verbatiminput{#1}}
}
\newcommand{\fggc}{Fast Genuine Generalized Consensus\ }

\begin{document}

\title{Personal Project Report\\
    Implementation and testing the \fggc (FGGC) for Networks-on-Chip}
\author{Motiejus Jak≈°tys}
\date{1 January 2013}

\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
\subsection{Identification}

This document is the report of Year 4 Personal Project named "Implement and
test the Fast Genuine Generalized Consensus algorithm for Networks-on-Chip".

\section{Outline}

What is FGGC?

FGGC belongs to the family of Paxos algorithms. Its characteristics are similar
to Fast Paxos, but with better recovery options.

From abstract[1]:

Consensus is a central primitive for building replicated systems, but its
latency constitutes a bottleneck. A well-known solution to consensus is Fast
Paxos. In a recent paper, Lamport enhances Fast Paxos by leveraging the
commutativity of concurrent commands. The new primitive, called Generalized
Paxos, reduces the collision rate, and thus the latency of Fast Paxos. However
if a collision occurs, the latency of Generalized Paxos equals six
communication steps, which is higher than Fast Paxos. This paper presents FGGC,
a novel consensus algorithm that reduces recovery delay when a collision occurs
to one.

\subsection{Motivation}

\subsubsection{Changes in many-core platforms}

During the last decade Moore's law had changed the rules of machine
performance. Clock speeds used to double every three years [reference needed].
However, recently frequency limits have been reached due to minimum possible
sizes of the chips. On the other hand, given clock speed limits, number of
cores on a chip started increasing rapidly. 8-core CPUs are now very common in
commodity desktop systems, and servers with 24 cores are more and more widely
deployed [reference needed].

Downside of current approach (SMP or NUMA) is that memory is shared among the
cores. Every core has its own memory cache, which must be consistent with
caches of other cores. This requirement increases communication overhead
between cores solely for cache coherency exponentially as number of cores
increases. Therefore for many cores (hundreds or thousands) another memory
management approach is needed.

Tilera(r?) is a multi-core machine producer which takes novel approach in
memory management. Main (DDR3) memory is shared amongst the cores, however,
what happens with caches is left completely up to the programmer to decide.  If
tasks in cores are separated and are working on different data sets, then, for
example, cache coherency can be turned off completely and, where
synchronization is necessary, program can benefit from User Data Network(?)
(UDN), which allows sending messages between cores with low latency and high
bandwidth (sending a [MSG SIZE] message to adjacent core takes only a few ticks,
which makes total bandwidth over 30Gb/s).

With this many-core approach is more natural to look at every core like to an
independent worker, or actor, and design the system which does not use shared
memory, but sends messages instead. Like discussed, sending messages and not
having the requirement to have consistent CPU caches decreases performance
penalty and allows the system to scale.

\subsubsection{Synchronization}

With huge number of small cores in the system (It is estimated that within 5
years a chip with 65536 cores will be produced [reference needed]), error
detection mechanisms becomes less sophisticated, and some computations might
become faulty. These faults in computations must be somehow detected by
software. This is a very good application for Paxos family consensus
algorithms.

[This statement looks a bit vague. I could try speaking about distributed
transactions instead, but do they make sense in 64k core machine?]

We decided to implement and evaluate the performance of \fggc on a 64-core
machine.

\subsection{Language Choice}

The language decision was based on these factors:
\begin{enumerate}
    \item Language can run on Tilera64
    \item How well is it suitable for algorithm implementation
    \item Performance
    \item Memory footprint
\end{enumerate}

Initially Haskell was chosen for the algorithm implementation. It is very
convenient to program state machines in a functional language; it compiles to
machine code, which makes it a good candidate for performance. Furthermore, it
has an llvm compiler, which is a promising sign for portability.

However, after some research it turned out that whole GHC must be ported to
Tilera64 to able to compile a program. Normal cross-compilation with GHC is not
possible, and cross-compilator for GHC was not the focus of the project, so it
was chosen not to be implemented.

Then we considered Erlang, because Erlang was specifically designed for
distributed systems, so it share the aims of our work, but is also a very
mature platform.

Erlang is a language based on the  {\em actor model}, characterized by the
following features:

{\em Concurrency:} Erlang has extremely lightweight processes whose memory
requirements can vary dynamically. Processes have no shared
memory\footnote{They do in some circumstances for efficiency reasons, but this
is completely transparent to the programmer} and communicate by asynchronous
message passing. 

{\em Distribution:} Erlang is designed to be run in a distributed
environment: it is as easy to create a process and communicate to
it on a host node like on a remote node.

Erlang is written in C and requires very standard tools to (cross-)compile and
run it. Supported platforms includes any Unix system, vxworks, Linux and
Windows. Older version of Erlang (R13B) has been cross-compiled and ran on
Tilera Multi-core Development Environment (MDE) version 2. Some patches to the
build system were necessary to compile newer version of Erlang (R15B02) on the
version 3 of Tilera MDE. They have been submitted upstream.

Like mentioned before, Erlang language is based on workers, or {\em actors},
doing different things and communicating with messages. This fits many-core
programming approach quite well.

\subsection{About Erlang Run Time System (ERTS)}

Under the hood Erlang virtual machine is a bytecode interpreter\footnote{for
x86 systems there is an optional native code compiler, which aims to improve
computational performance\cite{hipe}}. Erlang itself is an OS process, which
main building parts are {\em schedulers} and {\em processes}. Erlang processes
are light-weight (grow and shrink dynamically) with small memory footprint,
fast to create and terminate and the scheduling overhead is low. These are
scheduled by Erlang run-time system. {\em Scheduler} is an OS thread which
schedules the Erlang processes. By default Erlang creates as many schedulers as
there are cores available.

An Erlang {\em node} is an Erlang VM instance which can talk to other nodes.
Several nodes can be on different machines. Nodes can communicate over TCP, SSL
and unix pipes (default and most popular is TCP). Processes within a node can
transparently send messages to other processes (semantics are the same as if
they were sent from the same node), monitor other nodes or processes.

\section{Further topics}

These are left for Erlang part:
\begin{itemize}
    \item Erlang on Tilera64
        \subitem Node communication via UDN
        \subitem Which node distribution method was chosen and why
    \item Performance tests on merge sort and embarrassingly parallel program
    \item UPMARC and why we stopped trying to improve performance
\end{itemize}


\clearpage
\begin{thebibliography}{9}

    \bibitem{hipe} Konstantinos Sagonas, and Jesper Wilhelmsson \emph{Efficient
    memory management for concurrent programs that use message passing}.
    Science of Computer Programming, 62(2):98--121, October 2006.

\end{thebibliography}
\end{document}
