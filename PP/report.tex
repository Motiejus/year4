\documentclass[english,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{verbatim}
\usepackage{hyperref}
\newcommand{\listing}[1]{
    \subsubsection{#1}
    \label{lst:#1}
    {\small \verbatiminput{#1}}
}
\newcommand{\fggc}{Fast Genuine Generalized Consensus\ }

\begin{document}

\title{Personal Project Report\\
    Implementation and testing the \fggc (FGGC) for Networks-on-Chip}
\author{Motiejus Jak≈°tys}
\date{1 January 2013}

\maketitle
\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
\subsection{Identification}

This document is the report of Year 4 Personal Project.

\section{Rationale}

During the last decade Moore's law had changed the rules of machine
performance. Clock speeds used to double every three years [reference needed].
However, recently frequency limits have been reached due to minimum possible
sizes of the chips. On the other hand, given clock speed limits, number of
cores on a chip started increasing rapidly. 8-core CPUs are now very common in
commodity desktop systems, and servers with 24 cores are more and more widely
deployed [reference needed].

Future many-core systems in many aspects will look like distributed
many-computer systems nowadays. Computing units (cores) will be sending
messages to one another instead of relying on shared memory; this is already
done by current many-core systems manufacturer Tilera[reference]. Parallella is
foreseeing systems with 16K cores with similar power requirements of 16 core
nowadays systems [link or reference needed].

Since cores will be much smaller and less reliable, future many-core systems
need redundancy in order to be robust. Hence the reason for investigating
consensus algorithms in many-core systems.

\section{What we want to do}

Plan is to implement and test Fast Genuine Generalised Consensus algorithm on
Tilera64 many-core chip. Section~\ref{sec:paxos-family} presents what consensus
algorithms are. Section~\ref{sec:many-core} presents very recent changes of
many-core platforms and computing with them. Section~\ref{sec:manycore-paxos}
describes why consensus is important on many-core.
Section~\ref{sec:erlang-paxos} justifies the programming language used for
consensus implementation.

Section~\ref{sec:paxos-api} shows the API and architecture of classic paxos.
Section~\ref{sec:future} presents the future work.

\subsection{Consensus and Paxos family overview}
\label{sec:paxos-family}

Paxos family algorithms are designed for reliably reaching a consensus in a
distributed system. The most primitive application is reliably learning a value
by a majority of acceptors. It works as follows: a set of proposers propose a
value, and the algorithm guarantees that only one value will be chosen by a
majority of acceptors. In worse case no value will be chosen.

The most popular application is distributed state machine implementation: a
majority of cluster nodes can use this algorithm to agree on an order in which
to execute a series of commands. For example, this is useful to synchronize
database transactions across many database nodes which have local copies of
data. State machine replication is built on the primitive of choosing a single
value.

There is one family of algorithms that achieve this, the Paxos family.
Classical Paxos (the first version of Paxos) was first described by Leslie
Lamport in 1998, but gained attraction only at 2005. As we can see, it is a
fairly recent topic. [reference to Classic Paxos]. From then many variations of
the algorithm emerged.

Paxonian algorithms are generally used in state machine replication [make up a
reference]. One of possible optimizations is that some commands in the state
machine can be executed in different orders. For example, for two commands that
manipulate different unrelated accounts order is not important; having this in
mind some optimizations in consensus algorithm can be implemented.

Generalized Paxos has the optimization described above; Fast Paxos improves its
performance by [insert here], and Fast Genuine Generalized Consensus improves
Fast Paxos in case of conflicts.

\subsection{Overview of Fast Genuine Generalized Consensus (FGGC)}

FGGC belongs to the family of Paxos algorithms. Its characteristics are similar
to Fast Paxos, but with better recovery options.

From abstract[1]:

Consensus is a central primitive for building replicated systems, but its
latency constitutes a bottleneck. A well-known solution to consensus is Fast
Paxos. In a recent paper, Lamport enhances Fast Paxos by leveraging the
commutativity of concurrent commands. The new primitive, called Generalized
Paxos, reduces the collision rate, and thus the latency of Fast Paxos. However
if a collision occurs, the latency of Generalized Paxos equals six
communication steps, which is higher than Fast Paxos. This paper presents FGGC,
a novel consensus algorithm that reduces recovery delay when a collision occurs
to one.

\subsubsection{Changes in many-core platforms}
\label{many-core}

Tilera(r?) is a multi-core machine producer which takes novel approach in
memory management. Main (DDR3) memory is shared amongst the cores, however,
what happens with caches is left completely up to the programmer to decide.  If
tasks in cores are separated and are working on different data sets, then, for
example, cache coherency can be turned off completely and, where
synchronization is necessary, program can benefit from User Data Network(?)
(UDN), which allows sending messages between cores with low latency and high
bandwidth (sending a [MSG SIZE] message to adjacent core takes only a few ticks,
which makes total bandwidth over 30Gb/s).

With this many-core approach is more natural to look at every core like to an
independent worker, or actor, and design the system which does not use shared
memory, but sends messages instead. Like discussed, sending messages and not
having the requirement to have consistent CPU caches decreases performance
penalty and allows the system to scale.

\subsubsection{Synchronization}

We decided to implement and evaluate the performance of \fggc on a 64-core
machine.

\subsection{Language Choice}

The language decision was based on these factors:
\begin{enumerate}
    \item Language can run on Tilera64
    \item How well is it suitable for algorithm implementation
    \item Performance
    \item Memory footprint
\end{enumerate}

Initially Haskell was chosen for the algorithm implementation. It is very
convenient to program state machines in a functional language; it compiles to
machine code, which makes it a good candidate for performance. Furthermore, it
has an llvm compiler, which is a promising sign for portability.

However, after some research it turned out that whole GHC must be ported to
Tilera64 to able to compile a program. Normal cross-compilation with GHC is not
possible, and cross-compilator for GHC was not the focus of the project, so it
was chosen not to be implemented.

Then we considered Erlang, because Erlang was specifically designed for
distributed systems, so it share the aims of our work, but is also a very
mature platform.

Erlang is a language based on the  {\em actor model}, characterized by the
following features:

{\em Concurrency:} Erlang has extremely lightweight processes whose memory
requirements can vary dynamically. Processes have no shared
memory\footnote{They do in some circumstances for efficiency reasons, but this
is completely transparent to the programmer} and communicate by asynchronous
message passing.

{\em Distribution:} Erlang is designed to be run in a distributed
environment: it is as easy to create a process and communicate to
it on a host node like on a remote node.

Erlang is written in C and requires very standard tools to (cross-)compile and
run it. Supported platforms includes any Unix system, vxworks, Linux and
Windows. Older version of Erlang (R13B) has been cross-compiled and ran on
Tilera Multi-core Development Environment (MDE) version 2. Some patches to the
build system were necessary to compile newer version of Erlang (R15B02) on the
version 3 of Tilera MDE. They have been submitted upstream.

Like mentioned before, Erlang language is based on workers, or {\em actors},
doing different things and communicating with messages. This fits many-core
programming approach quite well.

\subsection{About Erlang Run Time System (ERTS)}

Under the hood Erlang virtual machine is a bytecode interpreter\footnote{for
x86 systems there is an optional native code compiler, which aims to improve
computational performance\cite{hipe}}. Erlang itself is an OS process, which
main building parts are {\em schedulers} and {\em processes}. Erlang processes
are light-weight (grow and shrink dynamically) with small memory footprint,
fast to create and terminate and the scheduling overhead is low. These are
scheduled by Erlang run-time system. {\em Scheduler} is an OS thread which
schedules the Erlang processes. By default Erlang creates as many schedulers as
there are cores available.

An Erlang {\em node} is an Erlang VM instance which can talk to other nodes.
Several nodes can be on different machines. Nodes can communicate over TCP, SSL
and unix pipes (default and most popular is TCP). Processes within a node can
transparently send messages to other processes (semantics are the same as if
they were sent from the same node), monitor other nodes or processes.

\section{Further topics}

These are left for Erlang part:
\begin{itemize}
    \item Erlang on Tilera64
        \subitem Node communication via UDN
        \subitem Which node distribution method was chosen and why
    \item Performance tests on merge sort and embarrassingly parallel program
    \item UPMARC and why we stopped trying to improve performance
\end{itemize}


\clearpage
\begin{thebibliography}{9}

    \bibitem{hipe} Konstantinos Sagonas, and Jesper Wilhelmsson \emph{Efficient
    memory management for concurrent programs that use message passing}.
    Science of Computer Programming, 62(2):98--121, October 2006.

\end{thebibliography}
\end{document}
